# ESM2AE ç¡¬ä»¶ä¼˜åŒ–é…ç½®æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨ä¸åŒç¡¬ä»¶é…ç½®é—´åˆ‡æ¢è®­ç»ƒé…ç½®ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

## ğŸ”¬ å½“å‰è®­ç»ƒé…ç½®

å½“å‰é…ç½®é’ˆå¯¹å¿«é€Ÿæµ‹è¯•å’Œæœ€å¤§æ˜¾å­˜åˆ©ç”¨è¿›è¡Œäº†ä¼˜åŒ–ï¼š
- **æ‰¹æ¬¡å¤§å°**: 
  - 2080 Super: 12 Ã— 4ç´¯ç§¯ Ã— 512åºåˆ— = 24,576 tokens
  - 3080: 32 Ã— 2ç´¯ç§¯ Ã— 512åºåˆ— = 32,768 tokens
- **å­¦ä¹ ç‡**: 4e-4 (å³°å€¼)ï¼Œ500æ­¥warmup (è®­ç»ƒæ­¥æ•°çš„1/10)ï¼Œä½™å¼¦è¡°å‡
- **ä¼˜åŒ–å™¨**: Adam (Î²1=0.9, Î²2=0.98, Îµ=1e-8, weight_decay=0.01)
- **è®­ç»ƒæ­¥æ•°**: 5000 steps (å¿«é€Ÿæµ‹è¯•ï¼ŒåŸå€¼500K)
- **åºåˆ—é•¿åº¦**: 512 tokens
- **æ˜¾å­˜åˆ©ç”¨**: å°½å¯èƒ½ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ˜¾å­˜

## æ”¯æŒçš„ç¡¬ä»¶é…ç½®

### 1. RTX 2080 Super å•å¡é…ç½® (ç”Ÿäº§è®­ç»ƒ)
- **ç¡¬ä»¶è§„æ ¼**: 1x RTX 2080 Super 8G VRAM + 64G RAM + 12æ ¸CPU
- **é…ç½®æ–‡ä»¶**: `train_config_2080super.yaml` + `ZERO2_2080super.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: å†…å­˜æ•ˆç‡ã€å¤§RAMåˆ©ç”¨ã€FP16æ··åˆç²¾åº¦

#### ä¸»è¦ç‰¹æ€§
- âœ… **FP16æ··åˆç²¾åº¦**: 2080 Superä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP16
- âœ… **æ¿€è¿›å†…å­˜ä¼˜åŒ–**: é’ˆå¯¹8Gæ˜¾å­˜é™åˆ¶
- âœ… **CPUå‚æ•°å¸è½½**: åˆ©ç”¨64G RAMä¼˜åŠ¿
- âœ… **å¤§RAMåˆ©ç”¨**: 12ä¸ªæ•°æ®åŠ è½½workerï¼Œ8å€é¢„å–å› å­
- âœ… **12æ ¸CPUä¼˜åŒ–**: å……åˆ†åˆ©ç”¨å¤šæ ¸CPUèµ„æº

#### æ€§èƒ½å‚æ•° (æœ€å¤§æ˜¾å­˜åˆ©ç”¨é…ç½®)
```yaml
per_device_train_batch_size: 12     # å°½å¯èƒ½ä½¿ç”¨æ˜¾å­˜ (åŸå€¼: 6)
gradient_accumulation_steps: 4      # å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼Œå› ä¸ºæ‰¹æ¬¡å¢å¤§äº†
max_length: 512                     # å›ºå®šåºåˆ—é•¿åº¦
learning_rate: 4e-4                 # ESM-2è®ºæ–‡å³°å€¼å­¦ä¹ ç‡
max_steps: 5000                     # å¿«é€Ÿæµ‹è¯• (åŸå€¼: 500000)
warmup_steps: 500                   # è®­ç»ƒæ­¥æ•°çš„1/10
lr_scheduler_type: "cosine"         # ä½™å¼¦è¡°å‡
```

### 2. RTX 3080 å•å¡é…ç½® (è°ƒè¯•å¼€å‘)
- **ç¡¬ä»¶è§„æ ¼**: 1x RTX 3080 10/12G VRAM + 12æ ¸CPU
- **é…ç½®æ–‡ä»¶**: `train_config_3080.yaml` + `ZERO2_3080.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: ç°ä»£GPUç‰¹æ€§ã€å¿«é€Ÿè¿­ä»£

#### ä¸»è¦ç‰¹æ€§
- âœ… **TF32 + BF16**: åˆ©ç”¨3080ç°ä»£ç‰¹æ€§
- âœ… **Flash Attention 2**: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- âœ… **Fusedä¼˜åŒ–å™¨**: æ›´å¿«çš„å‚æ•°æ›´æ–°
- âœ… **è°ƒè¯•å‹å¥½**: é¢‘ç¹æ—¥å¿—ã€æ€§èƒ½åˆ†æ
- âœ… **æ— CPUå¸è½½**: å•å¡æ— éœ€å¤æ‚å†…å­˜ç®¡ç†

#### æ€§èƒ½å‚æ•° (æœ€å¤§æ˜¾å­˜åˆ©ç”¨é…ç½®)
```yaml
per_device_train_batch_size: 32     # å°½å¯èƒ½ä½¿ç”¨æ˜¾å­˜ (åŸå€¼: 20)
gradient_accumulation_steps: 2      # å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼Œå› ä¸ºæ‰¹æ¬¡å¢å¤§äº†
max_length: 512                     # å›ºå®šåºåˆ—é•¿åº¦
learning_rate: 4e-4                 # ESM-2è®ºæ–‡å³°å€¼å­¦ä¹ ç‡
max_steps: 5000                     # å¿«é€Ÿæµ‹è¯• (åŸå€¼: 500000)
warmup_steps: 500                   # è®­ç»ƒæ­¥æ•°çš„1/10
lr_scheduler_type: "cosine"         # ä½™å¼¦è¡°å‡
tf32: true                          # å¯ç”¨TF32åŠ é€Ÿ
bf16: true                          # æ›´ç¨³å®šçš„æ··åˆç²¾åº¦
```

## å¿«é€Ÿåˆ‡æ¢é…ç½®

### ä½¿ç”¨é…ç½®åˆ‡æ¢è„šæœ¬

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é…ç½®
python train_config/switch_config.py --list

# åˆ‡æ¢åˆ°2080 Superé…ç½® (ç”Ÿäº§è®­ç»ƒ)
python train_config/switch_config.py --hardware 2080super

# åˆ‡æ¢åˆ°3080é…ç½® (è°ƒè¯•å¼€å‘)
python train_config/switch_config.py --hardware 3080

# æŸ¥çœ‹å½“å‰é…ç½®
python train_config/switch_config.py --current

# éªŒè¯é…ç½®æ–‡ä»¶
python train_config/switch_config.py --validate
```

### æ‰‹åŠ¨åˆ‡æ¢é…ç½®

å¦‚æœä¸ä½¿ç”¨è„šæœ¬ï¼Œå¯ä»¥æ‰‹åŠ¨å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š

```bash
# åˆ‡æ¢åˆ°2080 Superé…ç½®
cp train_config/train_config_2080super.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_2080super.yaml train_config/ZERO2_optimized.yaml

# åˆ‡æ¢åˆ°3080é…ç½®
cp train_config/train_config_3080.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_3080.yaml train_config/ZERO2_optimized.yaml
```

## æ€§èƒ½å¯¹æ¯”ä¸å»ºè®®

### RTX 2080 Super vs RTX 3080 å•å¡å¯¹æ¯”

| æŒ‡æ ‡ | 2080 Super | 3080 | è¯´æ˜ |
|------|------------|------|------|
| **æ˜¾å­˜** | 8GB | 10-12GB | 3080æ˜¾å­˜æ›´å¤§ |
| **å†…å­˜å¸¦å®½** | 448 GB/s | 760 GB/s | 3080å¸¦å®½æ›´é«˜ |
| **è®¡ç®—èƒ½åŠ›** | 11.15 TFLOPS | 29.77 TFLOPS | 3080è®¡ç®—èƒ½åŠ›æ›´å¼º |
| **ç°ä»£ç‰¹æ€§** | âŒ | âœ… TF32/BF16/FA2 | 3080æ”¯æŒæ›´å¤šä¼˜åŒ– |
| **ç³»ç»ŸRAM** | 64GB | æ ‡å‡†é…ç½® | 2080é…ç½®RAMæ›´å¤§ |

### ä½¿ç”¨å»ºè®®

#### é€‰æ‹©2080 Superé…ç½®çš„åœºæ™¯
- ğŸ¯ **ç”Ÿäº§è®­ç»ƒ**: é•¿æ—¶é—´ç¨³å®šè®­ç»ƒ
- ğŸ¯ **å¤§RAMä¼˜åŠ¿**: åˆ©ç”¨64Gç³»ç»Ÿå†…å­˜
- ğŸ¯ **å†…å­˜å¯†é›†**: å¤§æ‰¹æ¬¡æ•°æ®å¤„ç†
- ğŸ¯ **æˆæœ¬æ•æ„Ÿ**: å……åˆ†åˆ©ç”¨ç°æœ‰ç¡¬ä»¶

#### é€‰æ‹©3080é…ç½®çš„åœºæ™¯
- ğŸ¯ **å¿«é€ŸåŸå‹**: å¿«é€ŸéªŒè¯æƒ³æ³•
- ğŸ¯ **è°ƒè¯•å¼€å‘**: éœ€è¦è¯¦ç»†æ€§èƒ½åˆ†æ
- ğŸ¯ **å®éªŒæµ‹è¯•**: é¢‘ç¹ä¿®æ”¹æ¨¡å‹ç»“æ„
- ğŸ¯ **ç°ä»£ç‰¹æ€§**: éœ€è¦æœ€æ–°GPUç‰¹æ€§

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### 2080 Super å•å¡ä¼˜åŒ–

#### æ˜¾å­˜ä½¿ç”¨ä¼˜åŒ–
```bash
# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
nvidia-smi -l 1

# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°:
per_device_train_batch_size: 8      # ä»12å‡å°‘åˆ°8
gradient_accumulation_steps: 6      # ç›¸åº”å¢åŠ ç´¯ç§¯æ­¥æ•°
# æˆ–è€…å‡å°‘åºåˆ—é•¿åº¦:
max_length: 384                     # ä»512å‡å°‘åˆ°384
```

#### å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–
```bash
# å½“å‰ä½¿ç”¨ä½™å¼¦è¡°å‡ï¼Œå¯ä»¥è°ƒæ•´warmupæ¯”ä¾‹:
warmup_steps: 250                   # è®­ç»ƒæ­¥æ•°çš„1/20 (æ›´å¿«æ”¶æ•›)
warmup_steps: 1000                  # è®­ç»ƒæ­¥æ•°çš„1/5 (æ›´ç¨³å®š)
```

### 3080 å•å¡ä¼˜åŒ–

#### æ˜¾å­˜ä½¿ç”¨ä¼˜åŒ–
```bash
# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
nvidia-smi -l 1

# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°:
per_device_train_batch_size: 24     # ä»32å‡å°‘åˆ°24
gradient_accumulation_steps: 3      # ç›¸åº”å¢åŠ ç´¯ç§¯æ­¥æ•°

# ç°ä»£ç‰¹æ€§å¯ç”¨
pip install flash-attn --no-build-isolation
python -c "import torch; print(torch.backends.cuda.matmul.allow_tf32)"
```

#### è°ƒè¯•æ¨¡å¼
```yaml
# å¯ç”¨å¿«é€Ÿå¼€å‘æ¨¡å¼ (åœ¨é…ç½®æ–‡ä»¶ä¸­)
debug:
  fast_dev_run: true                # åªè¿è¡Œå‡ ä¸ªbatchæµ‹è¯•
  overfit_batches: 10               # è¿‡æ‹Ÿåˆå°‘é‡æ•°æ®éªŒè¯
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ˜¾å­˜ä¸è¶³ (OOM)
```bash
# 2080 Super: å‡å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size: 4

# æˆ–å¢åŠ æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps: 8

# æˆ–å‡å°‘åºåˆ—é•¿åº¦
max_length: 256
```

#### 2. è·¨å¡é€šä¿¡å¤±è´¥
```bash
# æ£€æŸ¥GPUæ‹“æ‰‘
nvidia-smi topo -m

# è®¾ç½®NCCLè°ƒè¯•
export NCCL_DEBUG=WARN
export NCCL_TREE_THRESHOLD=0
```

#### 3. Flash Attentioné”™è¯¯
```bash
# 3080: é™çº§åˆ°æ ‡å‡†attention
attn_implementation: "eager"

# æˆ–æ›´æ–°Flash Attention
pip install flash-attn --upgrade --no-build-isolation
```

### æ€§èƒ½ç›‘æ§

#### æ¨èç›‘æ§å·¥å…·
```bash
# GPUç›‘æ§
nvidia-smi -l 1

# ç³»ç»Ÿç›‘æ§  
htop

# ç½‘ç»œç›‘æ§ (åŒå¡é€šä¿¡)
iftop

# è®­ç»ƒç›‘æ§
wandb
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# å¯åŠ¨è®­ç»ƒå¹¶ç›‘æ§æ€§èƒ½
python3 train.py --config train_config/train_config_2080super.yaml

# å¯ç”¨åŸºå‡†æµ‹è¯•æ¨¡å¼
# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®: performance.benchmark_mode: true

# é¢„æœŸæ€§èƒ½æŒ‡æ ‡ (åºåˆ—é•¿åº¦512):
# 2080 Super x2: ~800 tokens/sec
# 3080 x1: ~900 tokens/sec (æ›´é«˜æ•ˆçš„æ¶æ„)
```

## é…ç½®æ–‡ä»¶è¯¦è§£

### å…³é”®å·®å¼‚å¯¹æ¯”

| é…ç½®é¡¹ | 2080 Super | 3080 | è¯´æ˜ |
|--------|------------|------|------|
| `tf32` | false | true | 3080æ”¯æŒTF32 |
| `fp16` | true | false | 2080ç”¨FP16ï¼Œ3080ç”¨BF16 |
| `bf16` | false | true | BF16æ•°å€¼æ›´ç¨³å®š |
| `attn_implementation` | "eager" | "flash_attention_2" | 3080æ”¯æŒFA2 |
| `optim` | "adamw_torch" | "adamw_torch_fused" | 3080ç”¨fusedä¼˜åŒ–å™¨ |
| `CPU offload` | å¯ç”¨ | ç¦ç”¨ | 2080éœ€è¦CPUå¸è½½ |

### è‡ªå®šä¹‰é…ç½®

å¦‚éœ€åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼Œå¯ä»¥åŸºäºç°æœ‰é…ç½®ä¿®æ”¹ï¼š

```bash
# å¤åˆ¶åŸºç¡€é…ç½®
cp train_config/train_config_3080.yaml train_config/my_custom_config.yaml

# ç¼–è¾‘é…ç½®
vim train_config/my_custom_config.yaml

# æ‰‹åŠ¨åˆ‡æ¢
cp train_config/my_custom_config.yaml train_config/trainer_config.yaml
```

---

## æ€»ç»“

é€šè¿‡ä½¿ç”¨é’ˆå¯¹ä¸åŒç¡¬ä»¶ä¼˜åŒ–çš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥ï¼š

1. **æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡**: æ¯ç§é…ç½®éƒ½é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–
2. **ç®€åŒ–åˆ‡æ¢æµç¨‹**: ä¸€é”®åˆ‡æ¢ä¸åŒç¯å¢ƒé…ç½®
3. **æå‡è®­ç»ƒæ•ˆç‡**: é¿å…é€šç”¨é…ç½®çš„æ€§èƒ½æŸå¤±
4. **é™ä½è°ƒè¯•æˆæœ¬**: è°ƒè¯•é…ç½®æä¾›æ›´å¤šè¯Šæ–­ä¿¡æ¯

å»ºè®®åœ¨å¼€å‘é˜¶æ®µä½¿ç”¨3080é…ç½®å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç”Ÿäº§è®­ç»ƒæ—¶åˆ‡æ¢åˆ°2080 Superé…ç½®ä»¥è·å¾—æ›´å¥½çš„æˆæœ¬æ•ˆç›Šã€‚