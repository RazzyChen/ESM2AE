# ESM2AE ç¡¬ä»¶ä¼˜åŒ–é…ç½®æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨ä¸åŒç¡¬ä»¶é…ç½®é—´åˆ‡æ¢è®­ç»ƒé…ç½®ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

## ğŸ”¬ ESM-2è®ºæ–‡è®­ç»ƒå‚æ•°

æ ¹æ®ESM-2è®ºæ–‡ï¼Œæˆ‘ä»¬çš„650Må‚æ•°æ¨¡å‹ä½¿ç”¨ä»¥ä¸‹å…³é”®è®¾ç½®ï¼š
- **æ‰¹æ¬¡å¤§å°**: 2M tokens (é€šè¿‡æ¢¯åº¦ç´¯ç§¯å®ç°)
- **å­¦ä¹ ç‡**: 4e-4 (å³°å€¼)ï¼Œ2000æ­¥warmupï¼Œç„¶åçº¿æ€§è¡°å‡åˆ°å³°å€¼çš„1/10
- **ä¼˜åŒ–å™¨**: Adam (Î²1=0.9, Î²2=0.98, Îµ=1e-8, weight_decay=0.01)
- **è®­ç»ƒæ­¥æ•°**: 500K steps
- **åºåˆ—é•¿åº¦**: 512 tokens (è®ºæ–‡ä¸­æåˆ°1024ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨512ä»¥é€‚åº”æ˜¾å­˜)
- **åˆ†å¸ƒå¼ç­–ç•¥**: 650Mæ¨¡å‹ä½¿ç”¨æ ‡å‡†DDP (ä¸éœ€è¦FSDP)

## æ”¯æŒçš„ç¡¬ä»¶é…ç½®

### 1. RTX 2080 Super å•å¡é…ç½® (ç”Ÿäº§è®­ç»ƒ)
- **ç¡¬ä»¶è§„æ ¼**: 1x RTX 2080 Super 8G VRAM + 64G RAM + 12æ ¸CPU
- **é…ç½®æ–‡ä»¶**: `train_config_2080super.yaml` + `ZERO2_2080super.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: å†…å­˜æ•ˆç‡ã€å¤§RAMåˆ©ç”¨ã€FP16æ··åˆç²¾åº¦

#### ä¸»è¦ç‰¹æ€§
- âœ… **FP16æ··åˆç²¾åº¦**: 2080 Superä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP16
- âœ… **æ¿€è¿›å†…å­˜ä¼˜åŒ–**: é’ˆå¯¹8Gæ˜¾å­˜é™åˆ¶
- âœ… **CPUå‚æ•°å¸è½½**: åˆ©ç”¨64G RAMä¼˜åŠ¿
- âœ… **å¤§RAMåˆ©ç”¨**: 12ä¸ªæ•°æ®åŠ è½½workerï¼Œ8å€é¢„å–å› å­
- âœ… **12æ ¸CPUä¼˜åŒ–**: å……åˆ†åˆ©ç”¨å¤šæ ¸CPUèµ„æº

#### æ€§èƒ½å‚æ•° (éµå¾ªESM-2è®ºæ–‡è®¾ç½®)
```yaml
per_device_train_batch_size: 6      # 512åºåˆ—é•¿åº¦ä¸‹8Gæ˜¾å­˜å®‰å…¨æ‰¹æ¬¡
gradient_accumulation_steps: 651    # æ¥è¿‘ESM-2çš„2M tokensæ‰¹æ¬¡å¤§å°
max_length: 512                     # å›ºå®šåºåˆ—é•¿åº¦
learning_rate: 4e-4                 # ESM-2è®ºæ–‡å³°å€¼å­¦ä¹ ç‡
max_steps: 500000                   # ESM-2è®ºæ–‡è®­ç»ƒæ­¥æ•°
adam_beta2: 0.98                    # ESM-2è®ºæ–‡ä¼˜åŒ–å™¨è®¾ç½®
weight_decay: 0.01                  # ESM-2è®ºæ–‡æƒé‡è¡°å‡
```

### 2. RTX 3080 å•å¡é…ç½® (è°ƒè¯•å¼€å‘)
- **ç¡¬ä»¶è§„æ ¼**: 1x RTX 3080 10/12G VRAM + 12æ ¸CPU
- **é…ç½®æ–‡ä»¶**: `train_config_3080.yaml` + `ZERO2_3080.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: ç°ä»£GPUç‰¹æ€§ã€å¿«é€Ÿè¿­ä»£

#### ä¸»è¦ç‰¹æ€§
- âœ… **TF32 + BF16**: åˆ©ç”¨3080ç°ä»£ç‰¹æ€§
- âœ… **Flash Attention 2**: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- âœ… **Fusedä¼˜åŒ–å™¨**: æ›´å¿«çš„å‚æ•°æ›´æ–°
- âœ… **è°ƒè¯•å‹å¥½**: é¢‘ç¹æ—¥å¿—ã€æ€§èƒ½åˆ†æ
- âœ… **æ— CPUå¸è½½**: å•å¡æ— éœ€å¤æ‚å†…å­˜ç®¡ç†

#### æ€§èƒ½å‚æ•° (éµå¾ªESM-2è®ºæ–‡è®¾ç½®)
```yaml
per_device_train_batch_size: 20     # 3080æ”¯æŒæ›´å¤§æ‰¹æ¬¡
gradient_accumulation_steps: 195    # æ¥è¿‘ESM-2çš„2M tokensæ‰¹æ¬¡å¤§å°
max_length: 512                     # å›ºå®šåºåˆ—é•¿åº¦
learning_rate: 4e-4                 # ESM-2è®ºæ–‡å³°å€¼å­¦ä¹ ç‡
tf32: true                          # å¯ç”¨TF32åŠ é€Ÿ
bf16: true                          # æ›´ç¨³å®šçš„æ··åˆç²¾åº¦
```

## å¿«é€Ÿåˆ‡æ¢é…ç½®

### ä½¿ç”¨é…ç½®åˆ‡æ¢è„šæœ¬

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é…ç½®
python train_config/switch_config.py --list

# åˆ‡æ¢åˆ°2080 Superé…ç½® (ç”Ÿäº§è®­ç»ƒ)
python train_config/switch_config.py --hardware 2080super

# åˆ‡æ¢åˆ°3080é…ç½® (è°ƒè¯•å¼€å‘)
python train_config/switch_config.py --hardware 3080

# æŸ¥çœ‹å½“å‰é…ç½®
python train_config/switch_config.py --current

# éªŒè¯é…ç½®æ–‡ä»¶
python train_config/switch_config.py --validate
```

### æ‰‹åŠ¨åˆ‡æ¢é…ç½®

å¦‚æœä¸ä½¿ç”¨è„šæœ¬ï¼Œå¯ä»¥æ‰‹åŠ¨å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š

```bash
# åˆ‡æ¢åˆ°2080 Superé…ç½®
cp train_config/train_config_2080super.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_2080super.yaml train_config/ZERO2_optimized.yaml

# åˆ‡æ¢åˆ°3080é…ç½®
cp train_config/train_config_3080.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_3080.yaml train_config/ZERO2_optimized.yaml
```

## æ€§èƒ½å¯¹æ¯”ä¸å»ºè®®

### RTX 2080 Super vs RTX 3080 å•å¡å¯¹æ¯”

| æŒ‡æ ‡ | 2080 Super | 3080 | è¯´æ˜ |
|------|------------|------|------|
| **æ˜¾å­˜** | 8GB | 10-12GB | 3080æ˜¾å­˜æ›´å¤§ |
| **å†…å­˜å¸¦å®½** | 448 GB/s | 760 GB/s | 3080å¸¦å®½æ›´é«˜ |
| **è®¡ç®—èƒ½åŠ›** | 11.15 TFLOPS | 29.77 TFLOPS | 3080è®¡ç®—èƒ½åŠ›æ›´å¼º |
| **ç°ä»£ç‰¹æ€§** | âŒ | âœ… TF32/BF16/FA2 | 3080æ”¯æŒæ›´å¤šä¼˜åŒ– |
| **ç³»ç»ŸRAM** | 64GB | æ ‡å‡†é…ç½® | 2080é…ç½®RAMæ›´å¤§ |

### ä½¿ç”¨å»ºè®®

#### é€‰æ‹©2080 Superé…ç½®çš„åœºæ™¯
- ğŸ¯ **ç”Ÿäº§è®­ç»ƒ**: é•¿æ—¶é—´ç¨³å®šè®­ç»ƒ
- ğŸ¯ **å¤§RAMä¼˜åŠ¿**: åˆ©ç”¨64Gç³»ç»Ÿå†…å­˜
- ğŸ¯ **å†…å­˜å¯†é›†**: å¤§æ‰¹æ¬¡æ•°æ®å¤„ç†
- ğŸ¯ **æˆæœ¬æ•æ„Ÿ**: å……åˆ†åˆ©ç”¨ç°æœ‰ç¡¬ä»¶

#### é€‰æ‹©3080é…ç½®çš„åœºæ™¯
- ğŸ¯ **å¿«é€ŸåŸå‹**: å¿«é€ŸéªŒè¯æƒ³æ³•
- ğŸ¯ **è°ƒè¯•å¼€å‘**: éœ€è¦è¯¦ç»†æ€§èƒ½åˆ†æ
- ğŸ¯ **å®éªŒæµ‹è¯•**: é¢‘ç¹ä¿®æ”¹æ¨¡å‹ç»“æ„
- ğŸ¯ **ç°ä»£ç‰¹æ€§**: éœ€è¦æœ€æ–°GPUç‰¹æ€§

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### 2080 Super åŒå¡ä¼˜åŒ–

#### å†…å­˜ä¼˜åŒ–
```bash
# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
nvidia-smi -l 1

# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°:
per_device_train_batch_size: 6      # å‡å°‘åˆ°6
max_length: 320                     # è¿›ä¸€æ­¥å‡å°‘åºåˆ—é•¿åº¦
gradient_accumulation_steps: 6      # ç›¸åº”å¢åŠ ç´¯ç§¯æ­¥æ•°
```

#### é€šä¿¡ä¼˜åŒ–
```bash
# ç¡®ä¿NCCLç¯å¢ƒå˜é‡è®¾ç½®æ­£ç¡®
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=^docker0,lo
export NCCL_IB_DISABLE=1            # ç¦ç”¨InfiniBand
```

### 3080 å•å¡ä¼˜åŒ–

#### ç°ä»£ç‰¹æ€§å¯ç”¨
```bash
# ç¡®ä¿Flash Attentionå®‰è£…
pip install flash-attn --no-build-isolation

# éªŒè¯TF32æ”¯æŒ
python -c "import torch; print(torch.backends.cuda.matmul.allow_tf32)"
```

#### è°ƒè¯•æ¨¡å¼
```yaml
# å¯ç”¨å¿«é€Ÿå¼€å‘æ¨¡å¼ (åœ¨é…ç½®æ–‡ä»¶ä¸­)
debug:
  fast_dev_run: true                # åªè¿è¡Œå‡ ä¸ªbatchæµ‹è¯•
  overfit_batches: 10               # è¿‡æ‹Ÿåˆå°‘é‡æ•°æ®éªŒè¯
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ˜¾å­˜ä¸è¶³ (OOM)
```bash
# 2080 Super: å‡å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size: 4

# æˆ–å¢åŠ æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps: 8

# æˆ–å‡å°‘åºåˆ—é•¿åº¦
max_length: 256
```

#### 2. è·¨å¡é€šä¿¡å¤±è´¥
```bash
# æ£€æŸ¥GPUæ‹“æ‰‘
nvidia-smi topo -m

# è®¾ç½®NCCLè°ƒè¯•
export NCCL_DEBUG=WARN
export NCCL_TREE_THRESHOLD=0
```

#### 3. Flash Attentioné”™è¯¯
```bash
# 3080: é™çº§åˆ°æ ‡å‡†attention
attn_implementation: "eager"

# æˆ–æ›´æ–°Flash Attention
pip install flash-attn --upgrade --no-build-isolation
```

### æ€§èƒ½ç›‘æ§

#### æ¨èç›‘æ§å·¥å…·
```bash
# GPUç›‘æ§
nvidia-smi -l 1

# ç³»ç»Ÿç›‘æ§  
htop

# ç½‘ç»œç›‘æ§ (åŒå¡é€šä¿¡)
iftop

# è®­ç»ƒç›‘æ§
wandb
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# å¯åŠ¨è®­ç»ƒå¹¶ç›‘æ§æ€§èƒ½
python3 train.py --config train_config/train_config_2080super.yaml

# å¯ç”¨åŸºå‡†æµ‹è¯•æ¨¡å¼
# åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®: performance.benchmark_mode: true

# é¢„æœŸæ€§èƒ½æŒ‡æ ‡ (åºåˆ—é•¿åº¦512):
# 2080 Super x2: ~800 tokens/sec
# 3080 x1: ~900 tokens/sec (æ›´é«˜æ•ˆçš„æ¶æ„)
```

## é…ç½®æ–‡ä»¶è¯¦è§£

### å…³é”®å·®å¼‚å¯¹æ¯”

| é…ç½®é¡¹ | 2080 Super | 3080 | è¯´æ˜ |
|--------|------------|------|------|
| `tf32` | false | true | 3080æ”¯æŒTF32 |
| `fp16` | true | false | 2080ç”¨FP16ï¼Œ3080ç”¨BF16 |
| `bf16` | false | true | BF16æ•°å€¼æ›´ç¨³å®š |
| `attn_implementation` | "eager" | "flash_attention_2" | 3080æ”¯æŒFA2 |
| `optim` | "adamw_torch" | "adamw_torch_fused" | 3080ç”¨fusedä¼˜åŒ–å™¨ |
| `CPU offload` | å¯ç”¨ | ç¦ç”¨ | 2080éœ€è¦CPUå¸è½½ |

### è‡ªå®šä¹‰é…ç½®

å¦‚éœ€åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼Œå¯ä»¥åŸºäºç°æœ‰é…ç½®ä¿®æ”¹ï¼š

```bash
# å¤åˆ¶åŸºç¡€é…ç½®
cp train_config/train_config_3080.yaml train_config/my_custom_config.yaml

# ç¼–è¾‘é…ç½®
vim train_config/my_custom_config.yaml

# æ‰‹åŠ¨åˆ‡æ¢
cp train_config/my_custom_config.yaml train_config/trainer_config.yaml
```

---

## æ€»ç»“

é€šè¿‡ä½¿ç”¨é’ˆå¯¹ä¸åŒç¡¬ä»¶ä¼˜åŒ–çš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥ï¼š

1. **æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡**: æ¯ç§é…ç½®éƒ½é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–
2. **ç®€åŒ–åˆ‡æ¢æµç¨‹**: ä¸€é”®åˆ‡æ¢ä¸åŒç¯å¢ƒé…ç½®
3. **æå‡è®­ç»ƒæ•ˆç‡**: é¿å…é€šç”¨é…ç½®çš„æ€§èƒ½æŸå¤±
4. **é™ä½è°ƒè¯•æˆæœ¬**: è°ƒè¯•é…ç½®æä¾›æ›´å¤šè¯Šæ–­ä¿¡æ¯

å»ºè®®åœ¨å¼€å‘é˜¶æ®µä½¿ç”¨3080é…ç½®å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç”Ÿäº§è®­ç»ƒæ—¶åˆ‡æ¢åˆ°2080 Superé…ç½®ä»¥è·å¾—æ›´å¥½çš„æˆæœ¬æ•ˆç›Šã€‚