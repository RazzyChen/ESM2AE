# ESM2AE ç¡¬ä»¶ä¼˜åŒ–é…ç½®æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨ä¸åŒç¡¬ä»¶é…ç½®é—´åˆ‡æ¢è®­ç»ƒé…ç½®ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

## æ”¯æŒçš„ç¡¬ä»¶é…ç½®

### 1. RTX 2080 Super åŒå¡é…ç½® (ç”Ÿäº§è®­ç»ƒ)
- **ç¡¬ä»¶è§„æ ¼**: 2x RTX 2080 Super 8G VRAM + 64G RAM (æ— NVLink)
- **é…ç½®æ–‡ä»¶**: `train_config_2080super.yaml` + `ZERO2_2080super.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: å†…å­˜æ•ˆç‡ã€è·¨å¡é€šä¿¡ä¼˜åŒ–

#### ä¸»è¦ç‰¹æ€§
- âœ… **FP16æ··åˆç²¾åº¦**: 2080 Superä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP16
- âœ… **æ¿€è¿›å†…å­˜ä¼˜åŒ–**: é’ˆå¯¹8Gæ˜¾å­˜é™åˆ¶
- âœ… **CPUå‚æ•°å¸è½½**: åˆ©ç”¨64G RAMä¼˜åŠ¿
- âœ… **è·¨å¡é€šä¿¡ä¼˜åŒ–**: æ— NVLinkç¯å¢ƒä¸‹çš„é€šä¿¡ç­–ç•¥
- âœ… **å¤§RAMåˆ©ç”¨**: 16ä¸ªæ•°æ®åŠ è½½workerï¼Œ8å€é¢„å–å› å­

#### æ€§èƒ½å‚æ•°
```yaml
per_device_train_batch_size: 8      # 8Gæ˜¾å­˜å®‰å…¨æ‰¹æ¬¡
gradient_accumulation_steps: 4      # è¡¥å¿å°æ‰¹æ¬¡å¤§å°
max_length: 384                     # å‡å°‘åºåˆ—é•¿åº¦èŠ‚çœæ˜¾å­˜
dataloader_num_workers: 16          # åˆ©ç”¨64G RAM
```

### 2. RTX 3080 å•å¡é…ç½® (è°ƒè¯•å¼€å‘)
- **ç¡¬ä»¶è§„æ ¼**: 1x RTX 3080 10/12G VRAM
- **é…ç½®æ–‡ä»¶**: `train_config_3080.yaml` + `ZERO2_3080.yaml`
- **ä¼˜åŒ–é‡ç‚¹**: ç°ä»£GPUç‰¹æ€§ã€å¿«é€Ÿè¿­ä»£

#### ä¸»è¦ç‰¹æ€§
- âœ… **TF32 + BF16**: åˆ©ç”¨3080ç°ä»£ç‰¹æ€§
- âœ… **Flash Attention 2**: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- âœ… **Fusedä¼˜åŒ–å™¨**: æ›´å¿«çš„å‚æ•°æ›´æ–°
- âœ… **è°ƒè¯•å‹å¥½**: é¢‘ç¹æ—¥å¿—ã€æ€§èƒ½åˆ†æ
- âœ… **æ— CPUå¸è½½**: å•å¡æ— éœ€å¤æ‚å†…å­˜ç®¡ç†

#### æ€§èƒ½å‚æ•°
```yaml
per_device_train_batch_size: 20     # 3080æ”¯æŒæ›´å¤§æ‰¹æ¬¡
gradient_accumulation_steps: 2      # å¿«é€Ÿåé¦ˆ
max_length: 512                     # æ”¯æŒæ›´é•¿åºåˆ—
tf32: true                          # å¯ç”¨TF32åŠ é€Ÿ
bf16: true                          # æ›´ç¨³å®šçš„æ··åˆç²¾åº¦
```

## å¿«é€Ÿåˆ‡æ¢é…ç½®

### ä½¿ç”¨é…ç½®åˆ‡æ¢è„šæœ¬

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é…ç½®
python train_config/switch_config.py --list

# åˆ‡æ¢åˆ°2080 Superé…ç½® (ç”Ÿäº§è®­ç»ƒ)
python train_config/switch_config.py --hardware 2080super

# åˆ‡æ¢åˆ°3080é…ç½® (è°ƒè¯•å¼€å‘)
python train_config/switch_config.py --hardware 3080

# æŸ¥çœ‹å½“å‰é…ç½®
python train_config/switch_config.py --current

# éªŒè¯é…ç½®æ–‡ä»¶
python train_config/switch_config.py --validate
```

### æ‰‹åŠ¨åˆ‡æ¢é…ç½®

å¦‚æœä¸ä½¿ç”¨è„šæœ¬ï¼Œå¯ä»¥æ‰‹åŠ¨å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š

```bash
# åˆ‡æ¢åˆ°2080 Superé…ç½®
cp train_config/train_config_2080super.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_2080super.yaml train_config/ZERO2_optimized.yaml

# åˆ‡æ¢åˆ°3080é…ç½®
cp train_config/train_config_3080.yaml train_config/trainer_config.yaml
cp train_config/ZERO2_3080.yaml train_config/ZERO2_optimized.yaml
```

## æ€§èƒ½å¯¹æ¯”ä¸å»ºè®®

### RTX 2080 Super åŒå¡ vs RTX 3080 å•å¡

| æŒ‡æ ‡ | 2080 Super x2 | 3080 x1 | è¯´æ˜ |
|------|---------------|---------|------|
| **æ˜¾å­˜** | 16GB (8+8) | 10-12GB | 2080åŒå¡æ€»æ˜¾å­˜æ›´å¤§ |
| **å†…å­˜å¸¦å®½** | 448 GB/s x2 | 760 GB/s | 3080å•å¡å¸¦å®½æ›´é«˜ |
| **è®¡ç®—èƒ½åŠ›** | 11.15 TFLOPS x2 | 29.77 TFLOPS | 3080è®¡ç®—èƒ½åŠ›æ›´å¼º |
| **ç°ä»£ç‰¹æ€§** | âŒ | âœ… TF32/BF16/FA2 | 3080æ”¯æŒæ›´å¤šä¼˜åŒ– |
| **è·¨å¡é€šä¿¡** | éœ€è¦ä¼˜åŒ– | æ— éœ€è€ƒè™‘ | å•å¡æ›´ç®€å• |

### ä½¿ç”¨å»ºè®®

#### é€‰æ‹©2080 Superé…ç½®çš„åœºæ™¯
- ğŸ¯ **ç”Ÿäº§è®­ç»ƒ**: é•¿æ—¶é—´ç¨³å®šè®­ç»ƒ
- ğŸ¯ **å¤§æ¨¡å‹è®­ç»ƒ**: éœ€è¦æ›´å¤šæ˜¾å­˜
- ğŸ¯ **å†…å­˜å¯†é›†**: å¤§æ‰¹æ¬¡æ•°æ®å¤„ç†
- ğŸ¯ **æˆæœ¬æ•æ„Ÿ**: å……åˆ†åˆ©ç”¨ç°æœ‰ç¡¬ä»¶

#### é€‰æ‹©3080é…ç½®çš„åœºæ™¯
- ğŸ¯ **å¿«é€ŸåŸå‹**: å¿«é€ŸéªŒè¯æƒ³æ³•
- ğŸ¯ **è°ƒè¯•å¼€å‘**: éœ€è¦è¯¦ç»†æ€§èƒ½åˆ†æ
- ğŸ¯ **å®éªŒæµ‹è¯•**: é¢‘ç¹ä¿®æ”¹æ¨¡å‹ç»“æ„
- ğŸ¯ **ç°ä»£ç‰¹æ€§**: éœ€è¦æœ€æ–°GPUç‰¹æ€§

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### 2080 Super åŒå¡ä¼˜åŒ–

#### å†…å­˜ä¼˜åŒ–
```bash
# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
nvidia-smi -l 1

# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°:
per_device_train_batch_size: 6      # å‡å°‘åˆ°6
max_length: 320                     # è¿›ä¸€æ­¥å‡å°‘åºåˆ—é•¿åº¦
gradient_accumulation_steps: 6      # ç›¸åº”å¢åŠ ç´¯ç§¯æ­¥æ•°
```

#### é€šä¿¡ä¼˜åŒ–
```bash
# ç¡®ä¿NCCLç¯å¢ƒå˜é‡è®¾ç½®æ­£ç¡®
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=^docker0,lo
export NCCL_IB_DISABLE=1            # ç¦ç”¨InfiniBand
```

### 3080 å•å¡ä¼˜åŒ–

#### ç°ä»£ç‰¹æ€§å¯ç”¨
```bash
# ç¡®ä¿Flash Attentionå®‰è£…
pip install flash-attn --no-build-isolation

# éªŒè¯TF32æ”¯æŒ
python -c "import torch; print(torch.backends.cuda.matmul.allow_tf32)"
```

#### è°ƒè¯•æ¨¡å¼
```yaml
# å¯ç”¨å¿«é€Ÿå¼€å‘æ¨¡å¼ (åœ¨é…ç½®æ–‡ä»¶ä¸­)
debug:
  fast_dev_run: true                # åªè¿è¡Œå‡ ä¸ªbatchæµ‹è¯•
  overfit_batches: 10               # è¿‡æ‹Ÿåˆå°‘é‡æ•°æ®éªŒè¯
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ˜¾å­˜ä¸è¶³ (OOM)
```bash
# 2080 Super: å‡å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size: 4

# æˆ–å¢åŠ æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps: 8

# æˆ–å‡å°‘åºåˆ—é•¿åº¦
max_length: 256
```

#### 2. è·¨å¡é€šä¿¡å¤±è´¥
```bash
# æ£€æŸ¥GPUæ‹“æ‰‘
nvidia-smi topo -m

# è®¾ç½®NCCLè°ƒè¯•
export NCCL_DEBUG=WARN
export NCCL_TREE_THRESHOLD=0
```

#### 3. Flash Attentioné”™è¯¯
```bash
# 3080: é™çº§åˆ°æ ‡å‡†attention
attn_implementation: "eager"

# æˆ–æ›´æ–°Flash Attention
pip install flash-attn --upgrade --no-build-isolation
```

### æ€§èƒ½ç›‘æ§

#### æ¨èç›‘æ§å·¥å…·
```bash
# GPUç›‘æ§
nvidia-smi -l 1

# ç³»ç»Ÿç›‘æ§  
htop

# ç½‘ç»œç›‘æ§ (åŒå¡é€šä¿¡)
iftop

# è®­ç»ƒç›‘æ§
wandb
```

#### æ€§èƒ½ç›‘æ§
```bash
# å¯åŠ¨è®­ç»ƒå¹¶ç›‘æ§æ€§èƒ½
python3 train.py --config train_config/train_config_2080super.yaml

# é¢„æœŸæ€§èƒ½æŒ‡æ ‡:
# 2080 Super x2: ~1000 tokens/sec
# 3080 x1: ~800 tokens/sec (ä½†å»¶è¿Ÿæ›´ä½)
```

## é…ç½®æ–‡ä»¶è¯¦è§£

### å…³é”®å·®å¼‚å¯¹æ¯”

| é…ç½®é¡¹ | 2080 Super | 3080 | è¯´æ˜ |
|--------|------------|------|------|
| `tf32` | false | true | 3080æ”¯æŒTF32 |
| `fp16` | true | false | 2080ç”¨FP16ï¼Œ3080ç”¨BF16 |
| `bf16` | false | true | BF16æ•°å€¼æ›´ç¨³å®š |
| `attn_implementation` | "eager" | "flash_attention_2" | 3080æ”¯æŒFA2 |
| `optim` | "adamw_torch" | "adamw_torch_fused" | 3080ç”¨fusedä¼˜åŒ–å™¨ |
| `CPU offload` | å¯ç”¨ | ç¦ç”¨ | 2080éœ€è¦CPUå¸è½½ |

### è‡ªå®šä¹‰é…ç½®

å¦‚éœ€åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼Œå¯ä»¥åŸºäºç°æœ‰é…ç½®ä¿®æ”¹ï¼š

```bash
# å¤åˆ¶åŸºç¡€é…ç½®
cp train_config/train_config_3080.yaml train_config/my_custom_config.yaml

# ç¼–è¾‘é…ç½®
vim train_config/my_custom_config.yaml

# æ‰‹åŠ¨åˆ‡æ¢
cp train_config/my_custom_config.yaml train_config/trainer_config.yaml
```

---

## æ€»ç»“

é€šè¿‡ä½¿ç”¨é’ˆå¯¹ä¸åŒç¡¬ä»¶ä¼˜åŒ–çš„é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥ï¼š

1. **æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡**: æ¯ç§é…ç½®éƒ½é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–
2. **ç®€åŒ–åˆ‡æ¢æµç¨‹**: ä¸€é”®åˆ‡æ¢ä¸åŒç¯å¢ƒé…ç½®
3. **æå‡è®­ç»ƒæ•ˆç‡**: é¿å…é€šç”¨é…ç½®çš„æ€§èƒ½æŸå¤±
4. **é™ä½è°ƒè¯•æˆæœ¬**: è°ƒè¯•é…ç½®æä¾›æ›´å¤šè¯Šæ–­ä¿¡æ¯

å»ºè®®åœ¨å¼€å‘é˜¶æ®µä½¿ç”¨3080é…ç½®å¿«é€Ÿè¿­ä»£ï¼Œåœ¨ç”Ÿäº§è®­ç»ƒæ—¶åˆ‡æ¢åˆ°2080 Superé…ç½®ä»¥è·å¾—æ›´å¥½çš„æˆæœ¬æ•ˆç›Šã€‚